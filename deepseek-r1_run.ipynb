{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek-R1 for OpenVINO GenAI 單獨執行含 Gradio 介面範例(deepseek-r1_run.ipynb)\n",
    "\n",
    "by Jack OmniXRI, 2025/03/24  \n",
    "\n",
    "原始範例: https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/deepseek-r1/deepseek-r1.ipynb  \n",
    "\n",
    "原始範例全部執行(Restart Kernel and Run All Cells）後，預設會取得  DeepSeek-R1-Distill-Qwen-14B 模型， INT4 權重壓縮格式，預計下載約 15 GB，轉檔後會產生 7.81 GB 大小檔案。如想要使用小一點的模型，可採用手動單步執行方式完成。   \n",
    "\n",
    "本範例為簡化版，執行前需先完整執行過原始範例，取得模型並轉換好 OpenVINO 所需 IR 格式。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 設定工作參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定執行裝置 \"CPU\", \"GPU\", \"NPU\", \"AUTO\"\n",
    "device = \"GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6452fb918543058211de05958bf31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Box(children=(Label(value='Language:'), Dropdown(index=1, options=('English', 'Chinese'), value=…"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手動切換下拉式選單，取得模型相關參數，包括語系、模型名稱、權重壓縮格式\n",
    "# 預設會指定 Chinese語系問題範例， DeepSeek-R1-Distill-Llama-8B 模型, INT4 權重壓縮格式\n",
    "# 相關內容可參考 llm_config_run.py\n",
    "\n",
    "from llm_config_run import get_llm_selection_widget\n",
    "\n",
    "form, lang, model_id_widget, compression_variant, _ = get_llm_selection_widget(device=device)\n",
    "\n",
    "# 顯示表單\n",
    "form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model DeepSeek-R1-Distill-Llama-8B with INT4 compression\n"
     ]
    }
   ],
   "source": [
    "# 將下拉式選單內容複製到變數中\n",
    "model_configuration = model_id_widget.value\n",
    "model_id = model_id_widget.label\n",
    "print(f\"Selected model {model_id} with {compression_variant.value} compression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定 OpenVINO IR 格式模型(*.xml)、權重(*.bin) 路徑，假設對應模型已下載並轉檔完成\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "model_path = f\"{model_id}\\\\{compression_variant.value}_compressed_weights\" \n",
    "model_dir = Path(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 使用 OpenVINO GenAI 實例化 LLM 流水線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from DeepSeek-R1-Distill-Llama-8B\\INT4_compressed_weights\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\openvino_env_py310\\lib\\site-packages\\openvino\\runtime\\__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading time: 8.4824 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openvino_genai as ov_genai\n",
    "import sys\n",
    "import time\n",
    "\n",
    "print(f\"Loading model from {model_dir}\\n\")\n",
    "\n",
    "t0 = time.perf_counter() # 取得開始時間\n",
    "\n",
    "pipe = ov_genai.LLMPipeline(str(model_dir), device) # 建立 LLM 流水線\n",
    "generation_config = ov_genai.GenerationConfig() # 設定基本參數\n",
    "generation_config.max_new_tokens = 128 # 設定最大新詞元數量\n",
    "\n",
    "t1 = time.perf_counter() # 取得結束時間\n",
    "print(f\"Model loading time: {(t1-t0):.4f} seconds\\n\") # 列出模型載入時間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 啟動聊天機器人人機介面\n",
    "\n",
    "Gradio 人機介面設定可參考 gradio_helper_deepseek_run.py  \n",
    "\n",
    "執行後可直接在欄位上操作，亦可開啟網址 http://127.0.0.1:7860 (http://localhost:7860) 使用瀏覽器頁面進行操作。  \n",
    "\n",
    "操作時可點選下方範例句子，或手動輸入問題，繁中/簡中/英文或混合皆可，接著按下「Summit」即可開始產生思考過程（簡中）及答案。如果顯示一半就停了，可點擊進階選項「Advance Options」。將最大新詞元「Max New Tokens」調到最大值即可改善。  \n",
    "\n",
    "如果不習慣簡體中文輸出，可輸入提示詞「以下內容請用繁體中文輸出」就可改善。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\openvino_env_py310\\lib\\site-packages\\gradio\\components\\chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gradio_helper_deepseek_run import make_demo\n",
    "\n",
    "demo = make_demo(pipe, model_configuration, model_id, lang.value, device == \"NPU\")\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True)\n",
    "except Exception:\n",
    "    demo.launch(debug=True, share=True)\n",
    "# If you are launching remotely, specify server_name and server_port\n",
    "# EXAMPLE: `demo.launch(server_name='your server name', server_port='server port in int')`\n",
    "# To learn more please refer to the Gradio docs: https://gradio.app/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/29454499/255799218-611e7189-8979-4ef5-8a80-5a75e0136b50.png",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "LLM"
    ],
    "tasks": [
     "Text Generation",
     "Conversational"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
